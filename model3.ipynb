{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras \n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Lambda, Dense, Flatten\n",
    "from keras.models import Model\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 928 images belonging to 2 classes.\n",
      "Found 107 images belonging to 2 classes.\n",
      "Found 535 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   validation_split=0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   validation_split=0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(r'C:\\projects_star\\fakeindiancurrency\\Indain Currency dataset\\train',\n",
    "                                                 target_size = (224, 224),\n",
    "                                                 batch_size = 8,\n",
    "                                                 subset=\"training\",\n",
    "                                                 class_mode = 'categorical')\n",
    "\n",
    "validation_set = train_datagen.flow_from_directory(r'C:\\projects_star\\fakeindiancurrency\\Indain Currency dataset\\test',\n",
    "                                                 target_size = (224, 224),\n",
    "                                                 batch_size = 8,\n",
    "                                                subset=\"validation\",\n",
    "                                                 class_mode = 'categorical')\n",
    "\n",
    "test_set = test_datagen.flow_from_directory(r'C:\\projects_star\\fakeindiancurrency\\Indain Currency dataset\\test',\n",
    "                                            target_size = (224, 224),\n",
    "                                            batch_size = 8,\n",
    "                                            class_mode = 'categorical')\n",
    "\n",
    "#color_mode = \"grayscale\"\n",
    "\n",
    "\n",
    "STEP_SIZE_TRAIN=training_set.n//training_set.batch_size\n",
    "STEP_SIZE_VALID=validation_set.n//validation_set.batch_size\n",
    "STEP_SIZE_TEST=test_set.n//test_set.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2)                 50178     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14764866 (56.32 MB)\n",
      "Trainable params: 50178 (196.01 KB)\n",
      "Non-trainable params: 14714688 (56.13 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "IMAGE_SIZE = [224, 224]\n",
    "vgg = VGG16(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)\n",
    "#here [3] denotes for RGB images(3 channels)\n",
    "\n",
    "#don't train existing weights\n",
    "for layer in vgg.layers:\n",
    " layer.trainable = False\n",
    " \n",
    "x = Flatten()(vgg.output)\n",
    "prediction = Dense(2, activation='sigmoid')(x)\n",
    "model = Model(inputs=vgg.input, outputs=prediction)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "                    optimizer=optimizers.Adam(),\n",
    "                    metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aksha\\AppData\\Local\\Temp\\ipykernel_7492\\3655621403.py:30: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  history = model.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 0.46127, saving model to mymodel1.h5\n",
      "Epoch 2/18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aksha\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2: val_loss improved from 0.46127 to 0.34786, saving model to mymodel1.h5\n",
      "Epoch 3/18\n",
      "\n",
      "Epoch 3: val_loss improved from 0.34786 to 0.23617, saving model to mymodel1.h5\n",
      "Epoch 4/18\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.23617\n",
      "Epoch 5/18\n",
      "\n",
      "Epoch 5: val_loss improved from 0.23617 to 0.18353, saving model to mymodel1.h5\n",
      "Epoch 6/18\n",
      "\n",
      "Epoch 6: val_loss improved from 0.18353 to 0.13996, saving model to mymodel1.h5\n",
      "Epoch 7/18\n",
      "\n",
      "Epoch 7: val_loss did not improve from 0.13996\n",
      "Epoch 8/18\n",
      "\n",
      "Epoch 8: val_loss improved from 0.13996 to 0.12054, saving model to mymodel1.h5\n",
      "Epoch 9/18\n",
      "\n",
      "Epoch 9: val_loss did not improve from 0.12054\n",
      "Epoch 10/18\n",
      "\n",
      "Epoch 10: val_loss did not improve from 0.12054\n",
      "Epoch 11/18\n",
      "\n",
      "Epoch 11: val_loss did not improve from 0.12054\n",
      "Epoch 12/18\n",
      "\n",
      "Epoch 12: val_loss improved from 0.12054 to 0.06850, saving model to mymodel1.h5\n",
      "Epoch 13/18\n",
      "\n",
      "Epoch 13: val_loss improved from 0.06850 to 0.05612, saving model to mymodel1.h5\n",
      "Epoch 14/18\n",
      "\n",
      "Epoch 14: val_loss did not improve from 0.05612\n",
      "Epoch 15/18\n",
      "\n",
      "Epoch 15: val_loss improved from 0.05612 to 0.04845, saving model to mymodel1.h5\n",
      "Epoch 16/18\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.04845\n",
      "Epoch 17/18\n",
      "\n",
      "Epoch 17: val_loss did not improve from 0.04845\n",
      "Epoch 18/18\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.04845\n",
      "Training completed in time:  0:21:05.246802\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
    "                               cooldown=0,\n",
    "                               patience=5,\n",
    "                               min_lr=0.5e-6)\n",
    "checkpoint = ModelCheckpoint(filepath='mymodel1.h5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "callbacks = [checkpoint, lr_reducer]\n",
    "start = datetime.now()\n",
    "# history = model.fit_generator(training_set, \n",
    "#                     steps_per_epoch=STEP_SIZE_TRAIN, \n",
    "#                     epochs = 18, verbose=5, \n",
    "#                     validation_data = validation_set, \n",
    "#                     validation_steps = STEP_SIZE_VALID)\n",
    "# duration = datetime.now() - start\n",
    "# print(\"Training completed in time: \", duration)\n",
    "\n",
    "# Assuming you have a `training_set` generator\n",
    "# and `batch_size` is the batch size used in the generator\n",
    "\n",
    "batch_size=8\n",
    "total_samples = len(training_set) * batch_size\n",
    "steps_per_epoch = total_samples // batch_size\n",
    "\n",
    "# Make sure steps_per_epoch is at least 1\n",
    "steps_per_epoch = max(steps_per_epoch, 1)\n",
    "\n",
    "history = model.fit_generator(\n",
    "    training_set,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    epochs=18,\n",
    "    verbose=5,\n",
    "    validation_data=validation_set,\n",
    "    validation_steps=len(validation_set),  # or calculate based on validation data size\n",
    "    callbacks=[checkpoint, lr_reducer]\n",
    ")\n",
    "duration = datetime.now() - start\n",
    "print(\"Training completed in time: \", duration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - 35s 523ms/step - loss: 0.0450 - accuracy: 0.9888\n",
      "Test Loss: 0.04496966674923897\n",
      "Test accuracy: 0.9887850284576416\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(test_set)\n",
    "print('Test Loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000273881D1DA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 258ms/step\n",
      "Predicted Label: Real\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "# Load the trained models\n",
    "model= load_model('mymodel1.h5')  # Trained model for spiral dataset\n",
    "# model_wave = load_model('mymodel_wave.h5')  # Trained model for wave dataset\n",
    "\n",
    "# Path to the new image\n",
    "new_image_path = r\"C:\\Users\\aksha\\Downloads\\test (3)_aug_1.jpg\" # Replace with the path to your new image\n",
    "\n",
    "# Load and preprocess the new image\n",
    "new_img = image.load_img(new_image_path, target_size=(224, 224))\n",
    "new_img_array = image.img_to_array(new_img)\n",
    "new_img_array = np.expand_dims(new_img_array, axis=0)\n",
    "new_img_array /= 255.0  # Normalize the image\n",
    "\n",
    "# Make predictions using both models\n",
    "prediction= model.predict(new_img_array)\n",
    "\n",
    "# Convert the combined predicted probabilities to class labels\n",
    "predicted_class = np.argmax(prediction)\n",
    "\n",
    "# Define the class labels\n",
    "class_labels = ['Fake', 'Real']\n",
    "\n",
    "# Get the label of the image\n",
    "predicted_label = class_labels[predicted_class]\n",
    "\n",
    "# Display the predicted label\n",
    "print(\"Predicted Label:\", predicted_label)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
